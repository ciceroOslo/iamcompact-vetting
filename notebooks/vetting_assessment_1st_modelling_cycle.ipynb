{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Run IAM COMPACT locally/interactively\n",
    "\n",
    " The code in this notebook shows how to run IAM COMPACT veting checks for the\n",
    " 1st modelling cycle manually and locally on the users' own computers, without\n",
    " being dependent on the I2AM PARIS web platform.\n",
    "\n",
    " The notebook comes in two formats:\n",
    "   * Extension `.ipynb`: A Jupyter notebook. Requires using a Jupyter notebook\n",
    "     server or a locally running Jupyter notebook kernel.\n",
    "   * Extension `.py`: A standard Python file with cell denoted in comments as\n",
    "     starting with `# %%`. Can be used interactively in Visual Studio Code or\n",
    "     other applications that can run Python files interactively and recognizes\n",
    "     the `# %%` cell delimiter. Can also be imported as a module, in which case\n",
    "     the entire file is run non-interactively from top to bottom, and outputs\n",
    "     become available as module attributes.\n",
    "\n",
    " This notebook is specific to 1st modelling cycle and the model result files\n",
    " that were available on the IAM COMPACT SharePoint in July 2024. This includes\n",
    " code to load and fix issues with those specific files. A separate notebbok\n",
    " file, named `vetting_assessment`, can be used if you want to run the code with\n",
    " other model results in hopefully non-problematic IAMC-formatted Excel file.\n",
    "\n",
    " In order to use this notebook, you will need to obtain a pickle file (used to\n",
    " store Python objects) with the results of the 1st modelling cycle. The file\n",
    " is called `data_dict.pkl` and must be placed in the folder\n",
    " `notebooks/cycle1_study_model_outputs`. You must have access to the IAM\n",
    " COMPACT SharePoint, and can find the file in the folder\n",
    " `Documents > General > 2-Deliverables and Milestones > WP4 (Modelling â€“\n",
    " Quantitative evidence in support of post-2030 Paris-compliant climate action) >\n",
    " First Modelling Cycle`. You can download the pickle file directly using the\n",
    " following link (provided you have access):\n",
    " [https://epuntuagr.sharepoint.com/:u:/r/sites/iamcompact/Shared%20Documents/General/2-Deliverables%20and%20Milestones/WP4%20(Modelling%20%E2%80%93%20Quantitative%20evidence%20in%20support%20of%20post-2030%20Paris-compliant%20climate%20action)/First%20Modelling%20Cycle/data_dict.pkl?csf=1&web=1&e=ehfa45](https://epuntuagr.sharepoint.com/:u:/r/sites/iamcompact/Shared%20Documents/General/2-Deliverables%20and%20Milestones/WP4%20(Modelling%20%E2%80%93%20Quantitative%20evidence%20in%20support%20of%20post-2030%20Paris-compliant%20climate%20action)/First%20Modelling%20Cycle/data_dict.pkl?csf=1&web=1&e=ehfa45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Setup\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Imports\n",
    "\n",
    " Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from collections.abc import Mapping\n",
    "\n",
    "import pyam\n",
    "import pandas as pd\n",
    "\n",
    "from iamcompact_vetting.targets.ar6_vetting_targets import (\n",
    "    vetting_targets as ar6_vetting_targets,\n",
    ")\n",
    "from iamcompact_vetting.targets.iamcompact_harmonization_targets import(\n",
    "    gdp_pop_harmonization_criterion,\n",
    "    IamCompactHarmonizationRatioCriterion,\n",
    "    IamCompactHarmonizationTarget,\n",
    ")\n",
    "from iamcompact_vetting.targets.target_classes import(\n",
    "    CriterionTargetRange,\n",
    ")\n",
    "from iamcompact_vetting.output.base import (\n",
    "    CriterionTargetRangeOutput,\n",
    "    MultiCriterionTargetRangeOutput,\n",
    ")\n",
    "from iamcompact_vetting.output.timeseries import (\n",
    "    TimeseriesRefFullComparisonOutput,\n",
    "    TimeseriesRefComparisonAndTargetOutput,\n",
    ")\n",
    "from iamcompact_vetting.output.excel import (\n",
    "    DataFrameExcelWriter,\n",
    "    MultiDataFrameExcelWriter,\n",
    "    make_valid_excel_sheetname,\n",
    ")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Set pandas display options\n",
    "\n",
    " We increase the number of rows displayed to make it easier to see full\n",
    " outputs. Decrease or increase as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.min_rows = 250\n",
    "pd.options.display.max_rows = 300\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Get the model/scenario data to be assessed.\n",
    "\n",
    " In the code cell below, add code to load the data you want to assess and\n",
    " assign it to the variable `iam_df`. This can be done either by using\n",
    " `pyam.IamDataFrame` to read from an Excel or CSV file, or by importing\n",
    " your own code that loads and/or processes the data.\n",
    "\n",
    " In this notebook specifically for the 1st modelling cycle, we use import a\n",
    " separate module that loads precompiled data from the Excel files with results\n",
    " from the 1st modelling cycle.\n",
    "\n",
    " *NB!* This will fail if you have not placed the pickle file 'data_dict.pkl'\n",
    " with the required data in the `notebooks/cycle1_study_model_outputs`\n",
    " subfolder. See the top of this file for explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycle1_study_model_outputs.cycle1_results import joint_iamdf\n",
    "\n",
    "iam_df: pyam.IamDataFrame = joint_iamdf\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data processing / fixing data issues\n",
    "\n",
    " In the code cell or cells below, add code to fix any errors in the data that\n",
    " you want to fix or do any necessary processing before running the vetting\n",
    " code. The variable `iam_df` must hold the correct data at the end. Add cells\n",
    " as needed, preferably at least one cell per distinct error being fixed.\n",
    "\n",
    " In this notebook for the 1st modelling cycle, there are several cells that\n",
    " make modifications to units, variable names and other aspects that needed to\n",
    " be adjusted to be compatible with the vetting procedures. Each distinct issue\n",
    " is processed in a separate cell under a distinct header.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Replace faulty unit `MtCO2/yr` with `Mt CO2/yr`\n",
    "\n",
    " The IAMC standard has a space between the mass unit and the gas species name\n",
    " for species-specific mass units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_df = iam_df.rename(\n",
    "    unit={\"MtCO2/yr\": \"Mt CO2/yr\"}\n",
    ")  # pyright: ignore[reportAssignmentType]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Replace `Carbon Capture` with `Carbon Sequestration|CCS` for PROMETHEUS\n",
    "\n",
    " The PROMETHEUS model uses `Carbon Capture` instead of the name\n",
    " `Carbon Sequestration|CCS` used by the `pathways-ensemble-analysis` package\n",
    " and the AR6 models. Rename it here to make sure that we can use\n",
    " `SingleVariableCriterion` for the \"CCS from energy\" vetting criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_CCS_df: pyam.IamDataFrame = iam_df.filter(\n",
    "    model='PROMETHEUS V1', variable='Carbon Capture*',  # pyright: ignore[reportAssignmentType]\n",
    ")\n",
    "other_df: pyam.IamDataFrame = iam_df.filter(\n",
    "    model='PROMETHEUS V1', \n",
    "    variable='Carbon Capture*',\n",
    "    keep=False,  # pyright: ignore[reportAssignmentType]\n",
    ")\n",
    "prometheus_rename_dict: dict[str, str] = {\n",
    "    _varname: _varname.replace(\"Carbon Capture\", \"Carbon Sequestration|CCS\")\n",
    "    for _varname in prometheus_CCS_df.variable  # pyright: ignore[reportAssignmentType]\n",
    "}\n",
    "prometheus_CCS_df = prometheus_CCS_df.rename(\n",
    "    variable=prometheus_rename_dict\n",
    ")  # pyright: ignore[reportAssignmentType]\n",
    "iam_df = pyam.concat([other_df, prometheus_CCS_df])\n",
    "\n",
    "if len(iam_df.filter(variable='Carbon Capture*').variable) > 0:\n",
    "    raise RuntimeError('Unexpected `Carbon Capture` variables remaining.')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Replace `Energy & Industrial Processes` with `Energy and Industrial Processes` in variable names.\n",
    "\n",
    " The IAMC standard uses \"and\" in variable names rather than \"&\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_df = iam_df.rename(\n",
    "    variable={\n",
    "        _varname: _varname.replace(\n",
    "            \"Energy & Industrial Processes\", \"Energy and Industrial Processes\"\n",
    "        )\n",
    "        for _varname in iam_df.variable\n",
    "        if 'Energy & Industrial Processes' in _varname\n",
    "    }\n",
    ")  # pyright: ignore[reportAssignmentType]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Define a new variable `Secondary Energy|Electricity|Wind and Solar`\n",
    "\n",
    " One of the AR6 vetting criteria require a single variable for electricity\n",
    " generated from wind and solar. This was not present in the 1st cycle models,\n",
    " so define it by adding up `Secondary Energy|Electricity|Wind` and\n",
    " `Secondary Energy|Electricity|Solar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_df = pyam.concat(\n",
    "    [\n",
    "        iam_df,\n",
    "        iam_df.add(\n",
    "            'Secondary Energy|Electricity|Wind',\n",
    "            'Secondary Energy|Electricity|Solar',\n",
    "            'Secondary Energy|Electricity|Wind and Solar',\n",
    "        )\n",
    "    ]\n",
    ")  # pyright: ignore[reportAssignmentType]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Correct GDP and population unit names\n",
    "\n",
    " GDP variables in the 1st modelling cycle data from some models used currency\n",
    " unit and base-year designations that are now considered non-standard, such\n",
    " as \"\\$US\" or \"US\\$\" instead of \"USD\", putting the base year directly after the\n",
    " currency unit rather than separating them by an underscore, and \"Billion\" with\n",
    " a capital \"B\" instead of \"billion\". The current, correct convention for IAMC\n",
    " formatted files is to use, e.g., \"USD_2010\" or \"USD_2017\" for 2010 and 2017\n",
    " US dollars.\n",
    "\n",
    " For population, some models used \"millions\" plural instead of \"million\", which\n",
    " also needs to be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_usd_unit_name(s: str) -> str:\n",
    "    usd_unit_name_pattern = re.compile(r\"(?:US\\$|\\$US)\\s*(\\d{4})\")\n",
    "    return usd_unit_name_pattern.sub(r\"USD_\\1\", s)\n",
    "\n",
    "iam_df = iam_df.rename(\n",
    "    unit={\n",
    "        _unit: _replace_usd_unit_name(_unit).replace(\"Billion\", \"billion\")\n",
    "        for _unit in iam_df.filter(variable='*GDP*').unit\n",
    "    } | {\n",
    "        'millions': 'million'\n",
    "    },\n",
    ")  # pyright: ignore[reportAssignmentType, reportOptionalMemberAccess]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Rename unspecified `GDP` variable\n",
    "\n",
    " The TIAM result files in the 1st modelling cycle uses a variable `GDP` without\n",
    " specifying whether it is MER or PPP. For vetting, we assume it's PPP and\n",
    " therefore rename it to `GDP|PPP` so that it will match the reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_df = iam_df.rename(variable={'GDP': 'GDP|PPP'})  # pyright: ignore[reportAssignmentType]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Assess the AR6 vetting ranges\n",
    "\n",
    " The cells below assess whether the results are in range and how far they are\n",
    " from the target value of each vetting criterion.\n",
    "\n",
    " The procedure uses `ar6_vetting_targets`, a list of `CriterionTargetRange`\n",
    " instances, each of which assesses `iam_df` against one of the AR6 vetting\n",
    " criteria. This list is used to produce a list of `MultiCriterionTargetRangeOutput`\n",
    " instance, which uses `vetting_targets`to produce output DataFrames, each of\n",
    " which are then written as a worksheet to an Excel file using a\n",
    " `MultiDataFrameExcelWriter` instance.\n",
    "\n",
    " The output Excel file will contain one worksheet for each vetting criterion.\n",
    " Each sheet has three index columns, with the name of a model/scenario pair\n",
    " in the first two, and the name of the vetting criterion in the third. The\n",
    " remaining columns are three value columns with vetting results:\n",
    "\n",
    " * `Is in target range`: A boolean value. `TRUE` if the model/scenario passes\n",
    "   the vetting criterion, `FALSE` otherwise.\n",
    " * `Rel. distance from target`: A measure of distance from the central target\n",
    "   value of the vetting criterion. The value is defined differently for each\n",
    "   criterion, and the exact value is not important, but it will generally be\n",
    "   between -1 and +1 for model/scenario pairs that pass the criterion, and\n",
    "   equal to 0 if it exactly hits the central value of the criterion. Values\n",
    "   very close to -1 or +1 indicate that the value of the vetted variable is\n",
    "   almost too low or too high to pass vetting.\n",
    " * `Value`: The value of the vetted variable. See the documentation of the AR6\n",
    "   vetting criteria for which variable or function of variables is evaluated in\n",
    "   each case.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First create a `pandas.ExcelWriter` instance which will write to the output\n",
    " Excel file. We need a common `pandas.ExcelWriter` instance for all of the\n",
    " vetting criteria, so that we can write results to different worksheets of the\n",
    " same Excel file.\n",
    "\n",
    " The cell below creates a `pandas.ExcelWriter` instance that will write to the\n",
    " file `vetting_results.xlsx` in the current working directory. Replace the file\n",
    " name with an alternative one or with a Python `Path` object if you wish to\n",
    " write to a differently named file or to a different directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_excel_writer: pd.ExcelWriter = \\\n",
    "    pd.ExcelWriter(\"ar6_vetting_results.xlsx\", engine='xlsxwriter')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then create the `MultiCriterionTargetRangeOutput` instance for the\n",
    " AR6 vetting criteria. The instance needs a `MultiDataFrameExcelWriter`\n",
    " instance # to write the results to different worksheets of the same Excel\n",
    " file. The # worksheets will have the same name as the corresponding vetting\n",
    " criterion, but with the name potentially shortened and with some characters\n",
    " substituted to make sure that they are valid names for Excel worksheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetting_results_output: MultiCriterionTargetRangeOutput[\n",
    "        CriterionTargetRange,\n",
    "        MultiDataFrameExcelWriter\n",
    "] = MultiCriterionTargetRangeOutput(\n",
    "    criteria={\n",
    "        make_valid_excel_sheetname(_crit.name): \\\n",
    "            _crit for _crit in ar6_vetting_targets},\n",
    "    writer=MultiDataFrameExcelWriter(results_excel_writer),\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, we call the `write_results` method of the\n",
    " `MultiCriterionTargetRangeOutput` instance, to compute the results and write\n",
    " them to the Excel file.\n",
    "\n",
    " The results are also returned as `pandas.DataFrame` objects in the dict\n",
    " `vetting_results_frames`, whose keys are equal to the Excel worksheet names\n",
    " (which in turn are equal to the names of the AR6 vetting criteria, shortened\n",
    " and modified to be valid Excel worksheet names).\n",
    "\n",
    " `results_excel_writer.close()` must be called at the end to close and save the\n",
    " Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetting_results: Mapping[str, pd.DataFrame]\n",
    "vetting_results, _ = vetting_results_output.write_results(iam_df)\n",
    "results_excel_writer.close()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Assess agreement with harmonisation data for population and GDP.\n",
    "\n",
    " The cells below will compare the model results in `iam_df` with the\n",
    " harmonization data for population and GDP in each region that is defined (has\n",
    " the same name) in both the harmonization data and in any of the models in\n",
    " `iam_df`. Note that it does not currently take into account differences in\n",
    " region definitions, or aggregate or translate model-specific region names used\n",
    " in different models. This is intended for a future version.\n",
    "\n",
    " The results are returned as a `pandas.DataFrame` and written to an Excel file\n",
    " with two worksheets:\n",
    " * `Ratios, full`: Shows the ratio between the values in `iam_df` relative to\n",
    "     the harmonization data, for each data point that exists in both data sets\n",
    "     for each year with no aggregation. Will be 1.0 values that are identical\n",
    "     to the harmonization data.\n",
    " * `Summary`: A table with summary results per model/scenario/region\n",
    "     combination. The table has two columns:\n",
    "     - `Pass`: Shows TRUE for model/scenario/region combinations that pass the\n",
    "       vetting criterion. This usually requires that all data points be within\n",
    "       2% of the target value, i.e., that the ratio is between 0.98 and 1.02.\n",
    "     - `Max rel. diff`: The maximum absolute relative difference between the\n",
    "       data value and harmonization data for the model/scenario/region\n",
    "       combination. The number given is `maxdiff - 1`, where `maxdiff` is\n",
    "       the ratio that differs most from 1.0 across all years. For example, if\n",
    "       the most deviant data value is 10% higher than the harmonization data,\n",
    "       `Max rel. diff` will be 0.1 (the ratio most different from 1.0 will be\n",
    "       1.1). If it is 15% lower than the harmonization data, `Max rel. diff`\n",
    "       will be -0.15 (the ratio most different from 1.0 will be 0.85).\n",
    "\n",
    " Generally, for population and GDP, the ratios should be between 0.98 and 1.02\n",
    " to be considered a close match. Values outside that range suggest that either\n",
    " the model/scenario has used data that do not agree with the harmonization\n",
    " data, or that there are issues with currency conversions, region definitions\n",
    " or other inconsistencies or mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get just the GDP and Population variables from the data. Assert that it\n",
    "# is not None (not necessary, but if you use Python with a type checker, it is\n",
    "# needed to avoid a warning, since the `IamDataFrame.filter` method can return\n",
    "# None):\n",
    "iam_df_pop_gdp = iam_df.filter(variable=['Population', 'GDP|PPP'])\n",
    "assert iam_df_pop_gdp is not None\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then create a `pandas.ExcelWriter` that later code will use to write to a\n",
    " specified Excel file. At the moment, it writes to the file\n",
    " `gdp_pop_harmonization_vetting.xlsx` in the current working directory, but\n",
    " you can change this to your liking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Test cell**. This should be removed in production, and moved further down to\n",
    " after the Excel part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pop_results_excel_writer: pd.ExcelWriter = pd.ExcelWriter(\n",
    "        'gdp_pop_harmonization_vetting.xlsx',\n",
    "        engine='xlsxwriter',\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then create a `DataFrameExcelWriter` instance that will do the actual writing\n",
    " to Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pop_harmonization_assessment_writer: MultiDataFrameExcelWriter = \\\n",
    "    MultiDataFrameExcelWriter(\n",
    "        file=gdp_pop_results_excel_writer,\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pop_harmonization_output: TimeseriesRefComparisonAndTargetOutput[\n",
    "    IamCompactHarmonizationRatioCriterion,\n",
    "    IamCompactHarmonizationTarget,\n",
    "    TimeseriesRefFullComparisonOutput,\n",
    "    CriterionTargetRangeOutput,\n",
    "    MultiDataFrameExcelWriter,\n",
    "    None\n",
    "] = TimeseriesRefComparisonAndTargetOutput(\n",
    "    criteria=gdp_pop_harmonization_criterion,\n",
    "    target_range=IamCompactHarmonizationTarget,\n",
    "    timeseries_output_type=TimeseriesRefFullComparisonOutput,\n",
    "    summary_output_type=CriterionTargetRangeOutput,\n",
    "    writer=gdp_pop_harmonization_assessment_writer\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pop_harmonization_result, _ignore = \\\n",
    "    gdp_pop_harmonization_output.write_results(iam_df_pop_gdp)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_pop_harmonization_assessment_writer.close()\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}